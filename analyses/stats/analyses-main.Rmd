---
fontsize: 11pt
geometry: margin=1.8cm
output:
  pdf_document:
    keep_tex: true
    toc: false
header-includes: |
    \usepackage[natbibapa, sectionbib, tocbib]{apacite}
    \usepackage{microtype}
    \usepackage[utf8]{inputenc}
    \usepackage{caption}
    \usepackage{lmodern}
    \usepackage{multirow}
    \usepackage{array}
    \usepackage{float}
    \usepackage[htt]{hyphenat}
    \usepackage{booktabs}
    \usepackage[euler]{textgreek}
    \usepackage{float}
    \usepackage[onehalfspacing]{setspace}
    \captionsetup[table]{width=\textwidth}
    \hypersetup{colorlinks = true, linkcolor = blue, urlcolor = red}
---

```{r setup, echo = F, include = F}
library(knitr)
opts_chunk$set(dpi = 600, dev = 'tikz', echo = F, include = F)
options(digits = 3, scipen = 12, knitr.kable.NA = '')

library(here)
# NOTE: this will load {magrittr}, {here}, {conflicted} and {tidyverse}. also,
# `conflict_prefer`s filter from {dplyr}
# furthermore, it loads 3 data.frames: (1) `dat` which contains the pooled data run
# through `2-wrangling-main.R`, (2) `datHard` which is `dat` with all the hard
# exclusion criteria applied (as described in `analysis-plan.md`), and (3)
# `datSoft` which is `datHard` with the soft exclusion criteria applied (as
# described in `analysis-plan.md`)
source(here('wrangling', '3-exclusion-criteria.R'))
# for different sums of squares
library(car)
# for tables
library(kableExtra)
# for effect sizes
library(compute.es)
# for bootstrapping
library(boot)
# for Bayesian analyses
library(BayesFactor)
# for extracting HDIs
library(HDInterval)

library(psych)

conflict_prefer('select', 'dplyr')
theme_set(theme_minimal())

# source helper functions
source(here('helpers', 'manova-helpers.R'))

apaPrint <- function(number, pvalue = F, digits = 3,
                     limit = .0001, replacement = '.0001') {
  if (pvalue & number < limit) {
    replacement
  } else {
    strCandidate <- round(number, digits = digits) %>%
    {str_replace(as.character(.), '(?<=[-+]?)0(?=\\.)', '')}
    if (str_length(strCandidate) < digits + 1) {
      str_pad(strCandidate, 'right', pad = '0', width = digits + 1)
    } else strCandidate
  }
}
```

## Results

### Exclusion criteria

```{r descTable, results = 'asis', include = T}
.a <- by(datHard$totalCorrect, datHard$condition, describe) %>%
  map_dfr(., rbind) %>%
  select(., n, mean, se, sd, min, max)
  
.b <- by(datHard$totalIntrusors, datHard$condition, describe) %>%
  map_dfr(., rbind) %>%
  select(., n, mean, se, sd, min, max)
  
rbind(.a, .b) %>%
  rename(., '$n$' = n, '$M$' = mean, '$SE_M$' = se, '$SD$' = sd) %>%
  cbind(Measure = c(rep('Total correct', 5), rep('Total intrusors', 5)),
        Condition = rep(c("Content, feedback", "Content, no feedback",
                          "General, feedback", "General, no feedback",
                          "Rereading")), .) %>% 
  kable(., caption = '\\label{descTable}Descriptive statistics for the DVs broken down
                     by experimental condition.',
        format = 'latex', booktabs = T, table.env = 'table*',
        digits = 3, escape = F) %>% 
  collapse_rows(., columns = 1, latex_hline = 'major') %>% 
  kable_styling(latex_options = c('hold_position'))
```

Prior to analysing the data, we have excluded participants based on a priori
set criteria. Participants who have spent less than or equal to 90 seconds
on the practice text were excluded
(`r nrow(dat) - dat %>% filter(., readingTime > 90) %>% nrow(.)` exclusion).
Further, we wanted to exclude participants who have had no correct answers on
the final test
(`r dat %>% filter(., readingTime > 90) %>% nrow(.) -
{dat %>% filter(., readingTime > 90 & totalCorrect > 0) %>% nrow(.)}` exclusions).
Finally, we have excluded participants who have stated that they have reading
deficits
(`r dat %>% filter(., readingTime > 90 & totalCorrect > 0) %>% nrow(.) -
{dat %>% filter(., readingTime > 90 & totalCorrect > 0 &
readingDeficits == 'NE')%>% nrow(.)}` exclusions). This left us with a total sample
of `r nrow(datHard)` participants. The descriptives for the sample are shown
in Table \ref{descTable}.
There is another set of exclusion criteria
based on the number of times the participants have read each of the three texts.
These are used in robustness check analyses (see supplementary materials).

### Interpolated activity effect

```{r datHardNofeedFilter}
datHardNofeed <- datHard %>% filter(., giveFeedback == F) %>%
  select(., activityFactor, totalCorrect, totalIntrusors)
datHardNofeed$activityFactor %<>% as.factor(.)
```

Our first two hypotheses are concerned with the effects of different interpolated
activities on the total number of correct answers and total number of intrusive
distractors chosen. To test these hypotheses, we have focused only on the groups
which have not received feedback (\(n\) = `r nrow(datHardNofeed)`).
This was done because there was no feedback option for the rereading group, and
we did not want to treat the feedback and no-feedback general-knowledge and
content-related testing groups as equivalent without strong evidence supporting
that assumption.
We conducted a one-way MANOVA with interpolated activity as
the independent variable and the total number of correct and intrusive options
chosen as dependent variables. The correlation between our DVs calculated on the
whole sample is \(r(`r nrow(datHard) - 2`) =\)
`r datHard %>% select(totalCorrect, totalIntrusors) %>% corr.test(.) -> .;
pluck(., 'r') %>% .[1,2] %>% apaPrint(.)`
(95% CI: [`r pluck(., 'ci') %>% pull(lower) %>% apaPrint(.)`,
`r pluck(., 'ci') %>% pull(upper) %>% apaPrint(.)`],
\(p\) < `r datHard %>% select(totalCorrect, totalIntrusors) %>% corr.test(.) -> .;
pluck(., 'p') %>% .[1,2] %>% apaPrint(., pvalue = T)`).

```{r manovaInterpActivity}
manovaModel <- manova(data = datHardNofeed,
                      cbind(totalCorrect, totalIntrusors) ~ activityFactor)
```

```{r manovaBoot, cache = T}
set.seed(1192017)

bootOmega <- function(data, i) {
  d <- data[i,]
  
  bootManova <- manova(data = d,
                       cbind(totalCorrect, totalIntrusors) ~
                         activityFactor)
  
  manovaSummary <- summary(bootManova, test = 'Wilks')
  
  omSq <- omegaSquared(nrow(bootManova$residuals), manovaSummary$stats[1, 'Wilks'],
               manovaSummary$stats[1, 'Df']) 
  
  return(omSq)
}

bootOut <- boot(datHardNofeed, statistic = bootOmega, R = 10000,
                parallel = 'multicore', ncpus = parallel::detectCores() - 1)

ciOut <- boot.ci(bootOut, index = 1)
```

Pillai's V for the analysis is
`r summary(manovaModel) %>% pluck(., 'stats') %>% .[1, 'Pillai'] %>% apaPrint(.)`,
\(p = `r summary(manovaModel) %>% pluck(., 'stats') %>% .[1, 'Pr(>F)'] %>%
apaPrint(., pvalue = T)`\)
(Wilks' \(\Lambda\) =
`r summary(manovaModel, test = 'Wilks') %>% pluck(., 'stats') %>% .[1, 'Wilks'] %>%
apaPrint(.)`,
\(p = `r summary(manovaModel, test = 'Wilks') %>% pluck(., 'stats') %>% .[1, 'Pr(>F)'] %>%
apaPrint(., pvalue = T)`\)).
The effect size, calculated as
\(\omega^2_{mult} = `r summary(manovaModel, test = 'Wilks') -> .;
omegaSquared(nrow(manovaModel$residuals), .$stats[1, 'Wilks'], .$stats[1, 'Df']) %>%
apaPrint(.)`\)
(bootstrap median\footnote{All bootstrap estimates taken from 10000 replications.} = `r bootOut$t %>% median(.) %>% apaPrint(.)`, \(BC_\alpha\) 95% CI =
[`r ciOut$bca[4] %>% apaPrint(.)`, `r ciOut$bca[5] %>% apaPrint(.)`]).
To further inspect the relationship of the interpolated activities with our
dependent variables, we have conducted a Roy-Bargmann stepdown analysis, as
suggested by \citeauthor{tabachnickUsingMultivariateStatistics2012}
(\citeyear{tabachnickUsingMultivariateStatistics2012};
a linear discriminant analysis with the same aim is available in the supplementary
materials). The total number of correct answers was a priori chosen to be the
higher priority variable. According to \citet{tabachnickUsingMultivariateStatistics2012},
the higher priority variable can be chosen based on theoretical or practical grounds.
Since, the total number of correct answers is the criterion that determines a
student's success in a testing context, we have chosen this dependent variable
as the higher priority one.
Therefore, we first conducted an ANOVA with interpolated
activity type as the independent variable and the total number of correct answers
as the dependent variable.

```{r royBargmann1}
# anova
anovaRB <- aov(totalCorrect ~ activityFactor, datHardNofeed)
anovaRB %<>% Anova(.) %>% broom::tidy(.)

# ancova with totalCorrect as covariate
ancovaRB <- aov(totalIntrusors ~ totalCorrect + activityFactor, datHardNofeed)
ancovaRB %>% Anova(., type = '3') %>% broom::tidy(.) -> .
```

As could be expected, the ANOVA points to an interpolated activity effect, with
\(F(`r anovaRB %>% filter(., term == 'activityFactor') %>% pull(., df)`,
`r anovaRB %>% filter(., term == 'Residuals') %>% pull(., df)`)\) =
`r anovaRB %>% select(., statistic) %>% slice(1)`,
\(p = `r anovaRB %>% select(., p.value) %>% slice(1) %>% apaPrint(., pvalue = T)`\).
Following the ANOVA, we conducted an ANCOVA, with
the total number of correct answers as the covariate, and the total number of intrusors
as the dependent variable. The results imply a main effect
of the total number of correct answers
(\(F(`r (.) %>% filter(., term == 'totalCorrect') %>% pull(df)`,
`r (.) %>% filter(., term == 'Residuals') %>% pull(df)`)\) =
`r (.) %>% filter(., term == 'totalCorrect') %>% pull(statistic)`,
\(p < `r (.) %>% filter(., term == 'totalCorrect') %>% pull(p.value) %>%
apaPrint(., pvalue = T)`\)),
but after taking into account the number of correct
answers, there is no evidence for an effect of interpolated activity on the total
number of chosen intrusors
(\(F (`r (.) %>% filter(., term == 'activityFactor') %>% pull(df)`,
`r (.) %>% filter(., term == 'Residuals') %>% pull(df)`)\) =
`r (.) %>% filter(., term == 'activityFactor') %>% pull(statistic)`,
\(p = `r (.) %>% filter(., term == 'activityFactor') %>% pull(p.value) %>%
apaPrint(., pvalue = T)`\)).
For now, we may claim that we do not have any evidence to support our second hypothesis
that the type of interpolated activity will have an effect on the number of
intrusors.

```{r contrastsH1}
contrastMatrix <- matrix(c(1, 1, -2, 1, -1, 0), 3, 2)
colnames(contrastMatrix) <- c('test vs rereading', 'content vs general')

anovaContrastH1 <- aov(totalCorrect ~ activityFactor, datHardNofeed,
                       contrasts = list(activityFactor = contrastMatrix))
anovaContrastH1 %>% summary.lm(.) %>% broom::tidy(.) -> .

esCont1 <- tes((.) %>% select(., statistic) %>% slice(2) %>% as.integer(.),
               datHardNofeed %>% tally(., activityFactor == 'rereading') %>%
                 as.integer(.),
               datHardNofeed %>% tally(., activityFactor != 'rereading') %>%
                 as.integer(.))

esCont2 <- tes((.) %>% select(., statistic) %>% slice(3) %>% as.integer(.),
               datHardNofeed %>% tally(., activityFactor == 'content') %>%
                 as.integer(.),
               datHardNofeed %>% tally(., activityFactor == 'general') %>%
                 as.integer(.))

```

In order to test our first hypothesis, we have contrasted
(i) the rereading group with the two test groups, and (ii) the two test groups with
each other, taking only the total number of correct answers as the DV.
The first contrast finds no evidence of a difference between the rereading group
and the two test groups
(\(t(`r summary.lm(anovaContrastH1) %>% pluck('df') %>% .[2]`)\) =
`r (.) %>% select(statistic) %>% slice(2)`,
\(p = `r (.) %>% select(p.value) %>% slice(2) %>% apaPrint(., pvalue = T)`\),
\(g_s\) = `r esCont1$g`, 95% CI = [`r esCont1$l.g`, `r esCont1$u.g`],
Cohen's \(U_{3, g_s}\) = `r esCont1$U3.g`%,
probability of superiority = `r esCont1$cl.g`%).
However, there is a difference between the two test groups
(\(t(`r summary.lm(anovaContrastH1) %>% pluck('df') %>% .[2]`)\) =
`r (.) %>% select(statistic) %>% slice(3)`,
\(p = `r (.) %>% select(p.value) %>% slice(3) %>% apaPrint(., pvalue = T, digits = 4)`\),
\(g_s\) = `r esCont2$g`, 95% CI = [`r esCont2$l.g`, `r esCont2$u.g`],
Cohen's \(U_{3, g_s}\) = `r esCont2$U3.g`%,
probability of superiority = `r esCont2$cl.g`%).
Participants in the content related test group scored higher on the final test
than participants in the general knowledge test condition.
These two findings are not in line with our predictions.

### The interaction between feedback and interpolated activity type

```{r datHardFeedFilter}
datHardFeed <- datHard %>% filter(., condition != 'rereading') %>%
  select(., condition, giveFeedback, activityFactor,
         totalCorrect, totalIntrusors)

datHardFeed %<>% mutate_at(., vars(activityFactor, giveFeedback, condition),
                           as.factor)
```

The remaining hypotheses deal with the effect of feedback on the total number of
correct answers and the total number of intrusors. Therefore, these analyses are
carried out only on the data from participants in the general and content related
test conditions (\(n\) = `r nrow(datHardFeed)`). 
To test these hypotheses, we first
conducted a two-way MANOVA with interpolated activity and feedback as independent
variables, and total number of correct answers and total number of intrusors as
the dependent variables.

```{r manovaInteraction}
manovaModelInter <- manova(cbind(totalCorrect, totalIntrusors) ~
                             activityFactor * giveFeedback,
                           data = datHardFeed)
```

```{r manovaBootActivity, cache = T}
set.seed(1912017)

bootOmegaActivity <- function(data, i) {
  d <- data[i,]
  
  bootManova <- manova(data = d,
                       cbind(totalCorrect, totalIntrusors) ~
                         activityFactor * giveFeedback)
  
  omSq <- omegaSquared(nrow(bootManova$residuals),
                       manovaStats(bootManova, 'test', '^activityFactor ',
                                   test = 'Wilks'),
                       manovaStats(bootManova, 'df', '^activityFactor ',
                                   test = 'Wilks')) 
  
  return(omSq)
}

bootOutActivity <- boot(datHardFeed, statistic = bootOmegaActivity,
                           R = 10000, parallel = 'multicore',
                           ncpus = parallel::detectCores() - 1)

ciOutActivity <- boot.ci(bootOutActivity, index = 1)
```

Pillai's V for the interpolated activity effect (calculated with type III sums of
squares) is 
`r manovaStats(manovaModelInter, 'test', '^activityFactor ') %>% apaPrint(.)`,
\(p = `r manovaStats(manovaModelInter, 'p', '^activityFactor ') %>% apaPrint(., pvalue = T)`\)
(Wilks' \(\Lambda\) =
`r manovaStats(manovaModelInter, 'test', '^activityFactor ', test = 'Wilks') %>%
apaPrint(.)`,
\(p = `r manovaStats(manovaModelInter, 'p', '^activityFactor ', test = 'Wilks') %>%
apaPrint(., pvalue = T)`\))
confirming the main effect of interpolated activity type.
The effect size
\(\omega^2_{mult}\) = `r omegaSquared(nrow(manovaModelInter$residuals),
manovaStats(manovaModelInter, 'test', '^activityFactor ', test = 'Wilks'),
manovaStats(manovaModelInter, 'df', '^activityFactor ', test = 'Wilks')) %>%
apaPrint(.)`
(bootstrap median = `r bootOutActivity$t %>% median(.) %>% apaPrint(.)`,
\(BC_\alpha\) 95% CI = [`r ciOutActivity$bca[4] %>% apaPrint(.)`,
`r ciOutActivity$bca[5] %>% apaPrint(.)`]).

```{r manovaBootFeed, cache = T}
set.seed(1171017)

bootOmegaFeed <- function(data, i) {
  d <- data[i,]
  
  bootManova <- manova(data = d,
                       cbind(totalCorrect, totalIntrusors) ~
                         activityFactor * giveFeedback)
  
  omSq <- omegaSquared(nrow(bootManova$residuals),
                       manovaStats(bootManova, 'test', '^giveFeedback ',
                                   test = 'Wilks'),
                       manovaStats(bootManova, 'df', '^giveFeedback ',
                                   test = 'Wilks')) 
  
  return(omSq)
}

bootOutFeed <- boot(datHardFeed, statistic = bootOmegaFeed,
                           R = 10000, parallel = 'multicore',
                           ncpus = parallel::detectCores() - 1)

ciOutFeed <- boot.ci(bootOutFeed, index = 1)
```

On the other hand, we find no evidence for an effect of giving feedback on the linear combination of
our two dependent variables --- Pillai's V =
`r manovaStats(manovaModelInter, 'test', '^giveFeedback ') %>% apaPrint(.)`,
\(p = `r manovaStats(manovaModelInter, 'p', '^giveFeedback ') %>% apaPrint(., pvalue = T)`\)
(Wilks' \(\Lambda\) =
`r manovaStats(manovaModelInter, 'test', '^giveFeedback ', test = 'Wilks') %>%
apaPrint(.)`,
\(p = `r manovaStats(manovaModelInter, 'p', '^giveFeedback ', test = 'Wilks') %>%
apaPrint(., pvalue = T)`\)).
The effect size is
\(\omega^2_{mult}\) = `r omegaSquared(nrow(manovaModelInter$residuals),
manovaStats(manovaModelInter, 'test', '^giveFeedback ', test = 'Wilks'),
manovaStats(manovaModelInter, 'df', '^giveFeedback ', test = 'Wilks')) %>%
apaPrint(.)`
(bootstrap median = `r bootOutFeed$t %>% median(.) %>% apaPrint(.)`\footnote{
The \(BC_\alpha\) 95\% CI for this estimate is \([`r ciOutFeed$bca[4] %>% apaPrint(.)`,
`r ciOutFeed$bca[5] %>% apaPrint(.)`]\).
\label{bca-ref}}).

```{r manovaBootInt, cache = T}
set.seed(1171717)

bootOmegaInt <- function(data, i) {
  d <- data[i,]
  
  bootManova <- manova(data = d,
                       cbind(totalCorrect, totalIntrusors) ~
                         activityFactor * giveFeedback)
  
  omSq <- omegaSquared(nrow(bootManova$residuals),
                       manovaStats(bootManova, 'test', '^activityFactor:',
                                   test = 'Wilks'),
                       manovaStats(bootManova, 'df', '^activityFactor:',
                                   test = 'Wilks')) 
  
  return(omSq)
}

bootOutInt <- boot(datHardFeed, statistic = bootOmegaInt,
                           R = 10000, parallel = 'multicore',
                           ncpus = parallel::detectCores() - 1)

ciOutInt <- boot.ci(bootOutInt, index = 1)
```

Furthermore, we find no evidence for an interaction effect between activity type
and feedback --- Pillai's V =
`r manovaStats(manovaModelInter, 'test', '^activityFactor:') %>% apaPrint(.)`,
\(p = `r manovaStats(manovaModelInter, 'p', '^activityFactor:') %>%
apaPrint(., pvalue = T)`\)
(Wilks' \(\Lambda\) =
`r manovaStats(manovaModelInter, 'test', '^activityFactor:', test = 'Wilks') %>%
apaPrint(.)`,
\(p = `r manovaStats(manovaModelInter, 'p', '^activityFactor:', test = 'Wilks') %>%
apaPrint(., pvalue = T)`\)).
The effect size
\(\omega^2_{mult}\) = `r omegaSquared(nrow(manovaModelInter$residuals),
manovaStats(manovaModelInter, 'test', '^activityFactor:', test = 'Wilks'),
manovaStats(manovaModelInter, 'df', '^activityFactor:', test = 'Wilks')) %>%
apaPrint(.)`
(bootstrap median = `r bootOutInt$t %>% median(.) %>% apaPrint(.)`\footnote{
The \(BC_\alpha\) 95\% CI = \([`r ciOutInt$bca[4] %>% apaPrint(.)`,
`r ciOutInt$bca[5] %>% apaPrint(.)`]\).
Our guess is that this odd result is due to the fact that most of the density is concentrated
around 0, causing an unreliable estimate. The same could be said for the CI in
footnote \ref{bca-ref}.}). Both the feedback and the interaction
estimates of \(\omega^2_{mult}\) are to be considered to be zero, given their negative values.

```{r royBargmann2}
anovaRB2 <- aov(totalCorrect ~ activityFactor * giveFeedback,
                data = datHardFeed)

Anova(anovaRB2, type = '3') %>% broom::tidy(.) -> rbAnova2

ancovaRB2 <- aov(totalIntrusors ~ activityFactor * giveFeedback * totalCorrect,
                 data = datHardFeed)

Anova(ancovaRB2, type = '3') %>% broom::tidy(.) -> rbAncova2
```

Again, we have conducted a follow-up Roy-Bargmann stepdown analysis. In the ANOVA
model with the total number of correct answers as the dependent variable and
the type of interpolated activity, feedback and their interaction as predictors,
only the type of activity seems to be relevant
(\(F(`r rbAnova2 %>% slice(2) %>% pull(df)`,
`r rbAnova2 %>% slice(5) %>% pull(df)`) =
`r rbAnova2 %>% slice(2) %>% pull(statistic)`, p =
`r rbAnova2 %>% slice(2) %>% pull(p.value) %>% apaPrint(., pvalue = T)`\)).
This result also shows that participants in the content related test condition
scored higher on the final test than the participants in the general knowledge test
condition, which should be no surprise given the results of the first stepdown analysis.
In the second step, we fit an ANCOVA model with the total number of correct answers
as the covariate. In this model, the type of interpolated activity ceases to be
a relevant predictor
(\(F(`r rbAncova2 %>% slice(2) %>% pull(df)`,
`r rbAncova2 %>% slice(9) %>% pull(df)`) =
`r rbAncova2 %>% slice(2) %>% pull(statistic)`, p
= `r rbAncova2 %>% slice(2) %>% pull(p.value) %>% apaPrint(., pvalue = T)`\)).
The full models are shown in Table \ref{rb2-table}.

```{r rb2Table, results = 'asis', include = T}
rbind(rbAnova2, rbAncova2) %>% filter(., !str_detect(term, 'Intercept')) %>%
  mutate_at(., vars(term), str_replace_all,
            'activityFactor', 'Activity') %>%
  mutate_at(., vars(term), str_replace_all,
            'giveFeedback', 'Feedback') %>%
  mutate_at(., vars(term), str_replace_all,
            'totalCorrect', 'Total correct') %>%
  mutate_at(., vars(term), str_replace_all,
            ':', ' x ') %>%
  modify_at(., 'p.value', map_chr,
            function(x) {
              ifelse(is.na(x), '', apaPrint(x, pvalue = T,
                                           replacement = '< .0001'))
            }) %>%
  rename('Term' = term, '$SS$' = sumsq, '$df$' = df,
              '$F$' = statistic, '$p$' = p.value) %>%
  kable(., caption = '\\label{rb2-table}ANOVA and ANCOVA models for the second Roy-Bargmann
                     procedure.',
        format = 'latex', booktabs = T, table.env = 'table*',
        digits = 3, escape = F, align = c('l', rep('r', 4))) %>%
  kable_styling(., latex_options = c('hold_position')) %>%
  pack_rows(index = c('ANOVA' = 4, 'ANCOVA' = 8))
```

## Additional analyses

Because it is theoretically interesting to see whether there is evidence for no
difference between certain conditions, or no effect of certain manipulations, we
have conducted a Bayesian reanalysis of the two Roy-Bargmann stepdown procedures.
Since these analyses were not planned, we have decided to use the default priors
provided in the \textit{BayesFactor}
\citep{moreyBayesFactorComputationBayes2018} package.\footnote{All posteriors obtained from 6000 simulations.}

```{r bfOpts}
numIter <- 6000
```

### Bayesian reanalysis of the first Roy-Bargmann procedure

```{r bfAnovaRB1, cache = T}
set.seed(1108)

anovaBfRb1 <- anovaBF(data = datHardNofeed,
                     totalCorrect ~ activityFactor)

anovaBfRb1Posterior <- posterior(anovaBfRb1, iterations = numIter)

anovaBfRb1Summary <- summary(anovaBfRb1Posterior)

anovaBfRb1HDI <- hdi(anovaBfRb1Posterior)
```

As was earlier done in a frequentist setting, we first fit an ANOVA model with the
total number of correct answers as the dependent variable, and the type of interpolated
activity as the predictor. The mean of the posterior intercept distribution is
`r anovaBfRb1Summary %>% pluck('statistics') %>% .[1, 'Mean']`
(95% highest density interval (HDI) =
[`r anovaBfRb1HDI[1, 'mu']`, `r anovaBfRb1HDI[2, 'mu']`]). The estimated mean of the
effect of content related testing is
`r anovaBfRb1Summary %>% pluck('statistics') %>% .[2, 'Mean']`
(95% HDI = [`r anovaBfRb1HDI[1, 'activityFactor-content']`,
`r anovaBfRb1HDI[2, 'activityFactor-content']`]). The 95% highest
density interval for the posterior indicates that there is a fair amount of uncertainty around
the exact magnitude of the effect of content-related testing. However, most of the probability density
is quite far above the null value, implying that we can be certain that there really
is a positive effect (given the used priors, of course). The means of the posterior
distributions for the general-knowledge-test and rereading conditions \(b\)s are
`r anovaBfRb1Summary %>% pluck('statistics') %>% .[3, 'Mean']`
(95% HDI = [`r anovaBfRb1HDI[1, 'activityFactor-general']`,
`r anovaBfRb1HDI[2, 'activityFactor-general']`])
and
`r anovaBfRb1Summary %>% pluck('statistics') %>% .[4, 'Mean']`,
(95% HDI = [`r anovaBfRb1HDI[1, 'activityFactor-rereading']`,
`r anovaBfRb1HDI[2, 'activityFactor-rereading']`])
respectively. Most of the posterior distribution for the effect of general knowledge
testing lies below the null value, although the distance is not as marked as in the
content-related condition. On the other hand, there is a lot of uncertainty
about the effect of rereading, compared to the other two estimates
(`r anovaBfRb1Posterior %>% as.data.frame(.) %>%
janitor::clean_names(.) %>%
select('activity_factor_rereading') %>%
filter(., activity_factor_rereading < 0) %>% nrow(.) / numIter * 100`% 
of the posterior lies below 0).

```{r bfRb1contrast, cache = T}
set.seed(9429)

datHardNofeed %>% filter(activityFactor != 'content') %>%
  ttestBF(data = .,
          formula = totalCorrect ~ activityFactor,
          posterior = T,
          iterations = numIter) -> contReadGen

hdi(contReadGen) -> contReadGenHDI
```

Furthermore, we wanted to explore the difference between the rereading and
general-knowledge-test conditions, given their somewhat similar coefficient
and HDI estimates. To do this, we conducted a Bayesian t-test, again with the
\textit{BayesFactor} package's default priors. The estimated posterior mean of the difference in
the total number of correct answers between the general-knowledge-test and
rereading groups is `r summary(contReadGen)$statistics[2, 'Mean']`
(95% HDI = [`r contReadGenHDI[1, 'beta (general - rereading)']`,
`r contReadGenHDI[2, 'beta (general - rereading)']`]). As can be seen from the
HDI, there is a lot of uncertainty around the estimate of the difference.
This points to a lack of evidence either for or against a difference between the
two conditions.

```{r, bfAncovaRB1, cache = T}
set.seed(171918)

ancovaBfRb1 <- lmBF(data = datHardNofeed,
                     totalIntrusors ~ activityFactor * totalCorrect)

ancovaBfRb1Posterior <- posterior(ancovaBfRb1, iterations = numIter)

ancovaBfRb1Summary <- summary(ancovaBfRb1Posterior)

ancovaBfRb1HDI <- hdi(ancovaBfRb1Posterior)

ancovaBfRb1Frame <- as.data.frame(ancovaBfRb1Posterior) %>%
  janitor::clean_names(.)
```
 
In the second step of the Roy-Bargmann procedure, we fit an ANCOVA model with
the total number of correct answers as the covariate and the total number
of intrusive options chosen as the dependent variable. The mean of the posterior
intercept distribution is `r ancovaBfRb1Summary$statistics[1, 'Mean']`
(95% HDI = [`r ancovaBfRb1HDI[1, 'mu']`,
`r ancovaBfRb1HDI[2, 'mu']`]). 
There is uncertainty around the estimates of the effects of the
different experimental conditions --- content related testing \(b\) =
`r ancovaBfRb1Summary$statistics[2, 'Mean']`
(95% HDI = [`r ancovaBfRb1HDI[1, 'activityFactor-content']`,
`r ancovaBfRb1HDI[2, 'activityFactor-content']`]),
general-knowledge testing \(b\) =
`r ancovaBfRb1Summary$statistics[3, 'Mean']`
(95% HDI = [`r ancovaBfRb1HDI[1, 'activityFactor-general']`,
`r ancovaBfRb1HDI[2, 'activityFactor-general']`]),
rereading \(b\) =
`r ancovaBfRb1Summary$statistics[4, 'Mean']`
(95% HDI = [`r ancovaBfRb1HDI[1, 'activityFactor-rereading']`,
`r ancovaBfRb1HDI[2, 'activityFactor-rereading']`]).
The HDIs show that the effects could be either slightly positive (decreasing the
number of intrusors) or slightly negative (increasing the number of intrusors),
preventing us from making a conclusion about the nature of the effects.
However, given the current data and priors, we find the following ---
`r ancovaBfRb1Frame %>% select(., activity_factor_content) %>%
filter(., activity_factor_content < 0) %>% nrow() / numIter * 100`% of the
posterior for the effect of content related testing falls below zero;
`r ancovaBfRb1Frame %>% select(., activity_factor_general) %>%
filter(., activity_factor_general > 0) %>% nrow() / numIter * 100`% 
of the posterior for the effect of general knowledge testing falls
above zero; `r ancovaBfRb1Frame %>% select(., activity_factor_rereading) %>%
filter(., activity_factor_rereading > 0) %>% nrow() / numIter * 100`% of
the posterior for the effect of rereading falls above zero.
Given the stated, there is some evidence implying that content related testing
decreases the number of intrusors chosen, after controlling for the effect of
the total number of correct answers. Further, there is some, albeit weaker evidence
that rereading leads to an increase in the number of chosen intrusive distractors.
Lastly, the posterior of the general knowledge testing effect points to no particular
direction.

### Bayesian reanalysis of the second Roy-Bargmann procedure

In the second Roy-Bargmann analysis, we wanted to test whether there is an effect
of the type of interpolated activity, receiving feedback, and their interaction
on the total number of correct answers and chosen intrusors. Again, we first fit
an ANOVA model with the two predictors and the total number of correct answers
as the dependent variable.

```{r bfAnovaRB2, cache = T}
set.seed(750363)

anovaBfRb2 <- anovaBF(data = datHardFeed,
                     totalCorrect ~ activityFactor * giveFeedback)

anovaBfRb2Posterior <- posterior(anovaBfRb2, iterations = numIter, index = 4)

anovaBfRb2Summary <- summary(anovaBfRb2Posterior)

anovaBfRb2HDI <- hdi(anovaBfRb2Posterior)
```

The mean of the posterior distribution of the intercept is
`r anovaBfRb2Summary$statistics[1, 'Mean']`
(95% HDI = [`r anovaBfRb2HDI[1, 'mu']`,
`r anovaBfRb2HDI[2, 'mu']`]). We find that being in the content-related-testing
condition leads to an increase in the total number of correct answers,
\(b\) = `r anovaBfRb2Summary$statistics[4, 'Mean']`
(95% HDI = [`r anovaBfRb2HDI[1, 'activityFactor-content']`,
`r anovaBfRb2HDI[2, 'activityFactor-content']`]), compared to the general-knowledge-testing
condition. This is aligned with the finding obtained in the frequentist setting.
The mean of the posterior for the effect of receiving feedback is
`r anovaBfRb2Summary$statistics[3, 'Mean']`
(95% HDI = [`r anovaBfRb2HDI[1, 'giveFeedback-TRUE']`,
`r anovaBfRb2HDI[2, 'giveFeedback-TRUE']`]). The HDI around the estimate prevents
us from making any relevant conclusions regarding the effect of receiving feedback.
However, we will mention that 
`r anovaBfRb2Posterior %>% as.data.frame() %>% janitor::clean_names() %>%
select(give_feedback_true) %>%
filter(give_feedback_true > 0) %>% nrow() / numIter * 100`% of the
posterior lies above zero.
Finally, the estimate for the interaction effect (being in the content condition
and receiving feedback) is
`r anovaBfRb2Summary$statistics[8, 'Mean']`
(95% HDI = [`r anovaBfRb2HDI[1, 'giveFeedback:activityFactor-TRUE.&.content']`,
`r anovaBfRb2HDI[2, 'giveFeedback:activityFactor-TRUE.&.content']`]). This could
point to there not being a relevant interaction effect. According to the collected
data and the priors, we could claim that the effect is practically equivalent to
zero if we were not interested in a half-point increase or decrease in the
average scores (i.e. defining a region of practical equivalence (ROPE) between [-0.5, 0.5]).
However, greater precision, which would require further data collection, is
desired.

```{r bfAncovaRB2, cache = T}
set.seed(174017)

ancovaBfRb2Posterior <- lmBF(data = datHardFeed,
                             totalIntrusors ~ activityFactor * totalCorrect *
                               giveFeedback,
                             posterior = T, iterations = numIter)

ancovaBfRb2Summary <- summary(ancovaBfRb2Posterior)

ancovaBfRb2HDI <- hdi(ancovaBfRb2Posterior)

ancovaBfRb2Frame <- as.data.frame(ancovaBfRb2Posterior) %>%
  janitor::clean_names(.)

```

We continue with the ANCOVA model, taking the total number of correct answers
as the covariate. The estimate of the model intercept is
`r ancovaBfRb2Summary$statistics[1, 'Mean']`
(95% HDI = [`r ancovaBfRb2HDI[1, 'mu']`, `r ancovaBfRb2HDI[2, 'mu']`]).
The estimate for the effect of content related testing on the total number
of intrusive distractors chosen is \(b\) = `r ancovaBfRb2Summary$statistics[2, 'Mean']`
(95% HDI = [`r ancovaBfRb2HDI[1, 'activityFactor-content']`,
`r ancovaBfRb2HDI[2, 'activityFactor-content']`]), compared to general knowledge
testing. There is some evidence for a
slight decrease in the number of intrusive distractors chosen in the
content related testing condition. However, an increase is
also possible, but less likely and negligibly small.
The estimate for the effect of receiving feedback is
`r ancovaBfRb2Summary$statistics[6, 'Mean']`
(95% HDI = [`r ancovaBfRb2HDI[1, 'giveFeedback-TRUE']`,
`r ancovaBfRb2HDI[2, 'giveFeedback-TRUE']`]).
Although the mean of the posterior is close to zero, the lower bound of the HDI
shows that values which may be considered non-negligible are still somewhat
probable. Therefore, we shall refrain from making a judgement regarding the effect
of feedback on choosing intrusive distractors.
Finally, the estimate of the interaction effect is \(b\) =
`r ancovaBfRb2Summary$statistics[10, 'Mean']`
(95% HDI = [`r ancovaBfRb2HDI[1, 'activityFactor:giveFeedback-content.&.TRUE']`,
`r ancovaBfRb2HDI[2, 'activityFactor:giveFeedback-content.&.TRUE']`]).
The mean of the posterior is close to zero, and we could declare the effect to be
practically equivalent to zero with a ROPE of approximately [-0.25, 0.25].

As previously stated, all these analyses were not planned a priori. This warrants
certain caveats. The \textit{BayesFactor} package's default priors were used.
The appropriateness of these priors should certainly be questioned. However, we have decided
to use them because we did not want to choose priors after already seeing the data.
Further, the statements about effects made in this section are noncommittal.
Whether a 0.5 increase or decrease in the total number of correct answers is
practically equivalent to zero or not is left to the reader.
We conclude by reminding the reader that the data is available online, at [URL].

## Notes

Analyses conducted using the \textit{R} language \citep{rcoreteamLanguageEnvironmentStatistical2019}.
Plots created using \textit{ggplot2} \citep{wickhamGgplot2ElegantGraphics2016}. Bootstrap conducted
using the \textit{boot} package \citep{cantyBootBootstrapSPlus2017}. Methods and analyses written
using \textit{rmarkdown} \citep{allaireRmarkdownDynamicDocuments2019} and
\textit{knitr} \citep{xieKnitrGeneralPurposePackage2019}. The package \textit{car}
\citep{foxCompanionAppliedRegression2011} was used to obtain type III sums of
squares. \textit{compute.es} \citep{reComputeEsCompute2013}
was used to obtain effect sizes for contrasts.
\textit{kableExtra} was used to help generate tables \citep{zhuKableExtraConstructComplex2019}.
Other utilities used are \textit{tidyverse} \citep{wickhamTidyverseEasilyInstall2017},
\textit{magrittr} \citep{bacheMagrittrForwardPipeOperator2014}, \textit{here}
\citep{mullerHereSimplerWay2017},
\textit{conflicted} \citep{wickhamConflictedAlternativeConflict2018},
\textit{psych} \citep{revellePsychProceduresPsychological2018}.
Highest density intervals obtained using \textit{HDInterval} \citep{meredithHDIntervalHighestPosterior2018}.
