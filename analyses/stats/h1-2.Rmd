---
author: "DV"
fontsize: 12pt
title: "H1-2"
output:
    pdf_document:
        keep_tex: true
        toc: true
        toc_depth: 4
geometry: margin=1.3cm
header-includes: |
    \usepackage[natbibapa, sectionbib, tocbib]{apacite}
    \usepackage[utf8]{inputenc}
    \usepackage{caption}
    \usepackage{bookman}
    \usepackage{multirow}
    \usepackage{array}
    \usepackage[htt]{hyphenat}
    \usepackage{booktabs}
    \usepackage[euler]{textgreek}
    \usepackage{float}
    \usepackage[onehalfspacing]{setspace}
    \captionsetup[table]{width=\textwidth}
    \hypersetup{colorlinks = true, linkcolor = blue, urlcolor = red}
---

```{r setup}
library(knitr)
library(kableExtra)
opts_chunk$set(dpi = 600, dev = 'pdf', echo = F)
options(digits = 5)
```

```{r packagesAndSource}
library(here)
# NOTE: this will load {magrittr}, {here}, {conflicted} and {tidyverse}. also,
# `conflict_prefer`s filter from {dplyr}
# furthermore, it loads 3 data.frames: (1) `dat` which contains the pooled data run
# through `2-wrangling-main.R`, (2) `datHard` which is `dat` with all the hard
# exclusion criteria applied (as described in `analysis-plan.md`), and (3)
# `datSoft` which is `datHard` with the soft exclusion criteria applied (as
# described in `analysis-plan.md`)
source(here('wrangling', '3-exclusion-criteria.R'))
# for Henze-Zirkler
library(MVN)
# for Box's M
library(heplots)
# colorscale
library(viridis)
# multivariate outliers
library(mvoutlier)
# for LDA
library(candisc)
library(MASS)
# for labels in ggplots
library(ggrepel)

conflict_prefer('select', 'dplyr')
theme_set(theme_minimal())

# source helper functions
source(here('helpers', 'h1-2-helpers.R'))
```

# Hard exclusion criteria

The following analyses are going to be conducted on a subset of the collected data
which contains `r nrow(datHard)` cases. First, we will take a look at the data going into
this analysis. Then, we will check whether the assumptions for conducting a MANOVA
are satisfied. Finally, we will conduct the analyses specified in the `analysis-plan.md`
file.

## Descriptive statistics

This analysis is going to be run on a subset participants who were in no-feedback
conditions. This includes the rereading group, and the two no-feedback test groups.

```{r}
# creating new df with the specified subset, selecting only relevant vars
datHardNofeed <- datHard %>% filter(., giveFeedback == F) %>%
  select(., activityFactor, totalCorrect, totalIntrusors)
```

This leaves us with `r nrow(datHardNofeed)` cases.

```{r}
datHardNofeed %>% glimpse(.)
```

First of all, it's important to mention that the group sizes are imbalanced, but the
difference is really small:

```{r}
datHardNofeed %>% group_by(., activityFactor) %>% tally(.)
```

Here are the descriptives for the whole subset.

```{r totalDescriptives}
datHardNofeed %>% select(., -activityFactor) %>% psych::describe(.)
```

And the descriptives by group:

```{r groupDescriptives}
by(datHardNofeed$totalCorrect, INDICES = datHardNofeed$activityFactor,
   psych::describe) %>% reduce(., rbind) %>%
  mutate(., condition = c("content", "general", "rereading")) %>%
  select(., condition, everything(), -vars)

by(datHardNofeed$totalIntrusors, INDICES = datHardNofeed$activityFactor,
   psych::describe) %>% reduce(., rbind) %>%
  mutate(., condition = c("content", "general", "rereading")) %>%
  select(., condition, everything(), -vars)
```

Now, we'll plot the DV distributions, both on the whole subset, and per group.

```{r densityTotal}
datHardNofeed %>% gather(., totalCorrect, totalIntrusors, key = 'dependentVar',
                         value = 'score') %>%
  ggplot(., aes(x = score)) +
  geom_density() + facet_grid(dependentVar ~ .)
```

The graph shows that there could be significant deviations from normality. Let's
look at the distributions in each group.

```{r densityGroup}
datHardNofeed %>% gather(., totalCorrect, totalIntrusors, key = 'dependentVar',
                         value = 'score') %>%
  ggplot(., aes(x = score, color = activityFactor, fill = activityFactor)) +
  geom_density(alpha = .5) + facet_grid(dependentVar ~ .) +
  scale_fill_viridis_d() + scale_color_viridis_d()
```

The distributions seem to be fairly similar on both dependent variables, and look
a bit more normal than on the whole sample.

Next, here are the boxplots for the three groups, and for both DVs.

```{r boxplots}
datHardNofeed %>% add_column(subject = 1:nrow(.), .before = 1) %>%
  gather(., totalCorrect, totalIntrusors, key = 'dependentVar',
         value = 'score') %>%
  ggplot(., aes(x = activityFactor, y = score, fill = activityFactor)) +
  geom_boxplot(alpha = .5) + geom_dotplot(mapping = aes(y = score),
                                          binaxis = 'y',
                                stackdir = 'centerwhole', dotsize = .5,
                                fill = 'black', color = 'black') +
  facet_grid(dependentVar~.) + scale_fill_viridis(discrete = T)
```

Looks like there could be an outlier on the number of total intrusions in the
general knowledge test condition.

## Assumption checks

Next, let's look at the correlation between the DVs. First, let's look at the correlation
in the whole sub-sample.

```{r dvCorTotal}
. <- datHardNofeed %>% select(., -activityFactor) %>% psych::corr.test(.)
print(.)
```

We found a statistically significant correlation of `r .$r[2]`. Next, let's look at
the correlation in each group.

```{r dvCorGroup}
by(datHardNofeed[, c('totalCorrect', 'totalIntrusors')],
   INDICES = datHardNofeed$activityFactor, FUN = psych::corr.test)
```

The correlations in all three groups are fairly similar and statistically significant.
To check whether the relationship between the DVs could be described as linear,
we'll plot the scatterplots for the whole sample and for each group.

```{r scatterToral}
datHardNofeed %>%
  ggplot(., aes(x = totalCorrect, y = totalIntrusors)) +
  geom_point(aes(fill = activityFactor, color = activityFactor)) +
  geom_smooth(method = 'lm', color = 'black', level = .99) +
  scale_fill_viridis_d(option = 'D') + scale_color_viridis_d(option = 'D')
```

As can be seen from the plot, the relationship seems to be pretty linear, and the
99% confidence interval of the regression slopes is pretty small. Now, let's
take a look at the scatterplots for each group separately.

```{r scatterGroup}
datHardNofeed %>%
  ggplot(., aes(x = totalCorrect, y = totalIntrusors)) +
  geom_point(aes(fill = activityFactor, color = activityFactor)) +
  geom_smooth(method = 'lm', level = .99, color = 'black') +
  facet_grid(activityFactor ~ .) +
  scale_fill_viridis_d(option = 'D') + scale_color_viridis_d(option = 'D')
```

Again, we can see that the relationships are linear, as well as similar. Next,
we will check the multivariate normality assumption using the Henze-Zirkler test.

```{r hzTest}
by(datHardNofeed %>% select(-activityFactor),
   INDICES = datHardNofeed$activityFactor,
   FUN = mvn, mvnTest = 'hz')
```

The result of Henze-Zirkler's multivariate normality test shows that a statistically
significant departure from multivariate normality was not detected. Hence, we will
assume that there really is no departure. The points on the Chi-Square Q-Q plot
follow the straight line fairly well, which is also indicative of a normal
distribution.

However, the Shapiro-Wilk test for univariate normality indicates a departure in
the distribution of the total number of intrusive distractors chosen in one group. It is interesting
to notice that the value of the *W* statistic is close to the maximum of 1, which
indicates a close fit to the normal distribution \citep{salkind_encyclopedia_2007}.
Also, the test is only marginally significant at the conventional .05 level.
Therefore, we will assume that the data in each group is normally distributed.

Now, we'll take a look at the homogeneity of covariance matrices assumption. First,
let's take a look at the matrices themselves.

```{r covMatricesGroup}
by(datHardNofeed[, c('totalCorrect', 'totalIntrusors')],
   INDICES = datHardNofeed$activityFactor, FUN = cov) -> .
print(.)
```

The covariance matrices look quite similar. The ratio of largest to smallest
variance of the number of correct answers is `r covMatrixRatio(1, 1, .)`. The same
ratio for the number of chosen intrusors is `r covMatrixRatio(2, 2, .)`. Finally,
the ratio of the largest to the smallest covariance between the two DVs is
`r covMatrixRatio(1, 2, .)`. As we can see, the variance ratios are pretty close
to one. Still, let's test them with Box's M test.

```{r boxM}
boxM(datHardNofeed %>% select(., -activityFactor),
     group = datHardNofeed$activityFactor)
```

Box's M returns a non-significant p-value, indicating that we cannot reject the null
hypothesis. \citet{field_discovering_2012} and \citet{raykov_introduction_2008}
warn that Box's M is extremely sensitive, so we'd expect to find a difference
if there really was one. On the other hand, \citet{field_discovering_2012} warn
that the test can return a non-significant p-value when the assumption of
multivariate normality is not tenable. However, given the results of the
Henze-Zirkler multivariate normality test, we suspect that this is not the case.
Since all assumptions seem to hold, we can proceed with the planned MANOVA.

### Note

A decision was made not to check the univariate and multivariate outliers at this
point. Regarding the univariate outliers - the boxplots point to only one case
which could be an outlier. The scatterplots show no point that's obviously different
from the rest. As for the multivariate outliers, \citet{tabachnick_using_2012} warn
that the Mahalanobis distance can produce false negatives or false positives. Furthermore,
deleting a set of outliers and rerunning the analysis can reveal yet another set of
outliers --- without a clear-cut and absolute criterion, exclusions are somewhat
arbitrary. Finally, cases were excluded based on criteria that are more or less
substantivelly meaningful in the context of the conducted study. Given the above,
no statistical criteria is used for exclusion at this point.

## MANOVA

Now that it seems that all the assumptions of a MANOVA are satisfied, let's run
the analysis.

```{r manova}
manovaModel <- manova(data = datHardNofeed,
                      cbind(totalCorrect, totalIntrusors) ~
                        as.factor(activityFactor))
summary(manovaModel)
summary(manovaModel, test = 'Wilks')
summary(manovaModel, test = 'Hotelling')
summary(manovaModel, test = 'Roy')
```

As can be seen from the resulting output, Pillai's V indicates that the three
groups differ significantly along the linear combination of the two DVs. The other
three reported statistics point to the same conclusion. Therefore, we'll proceed
with conducting a linear discriminant analyis.

We can look at \(1 - \Lambda\) as an extension to the univariate \(\eta^2\)
\citep{huberty_applied_2006}. In our case, the multivariate \(\eta^2\) is
`r 1 - summary(manovaModel, test = 'Wilks') %>% pluck('stats') %>% .[1,2]`, which
represents the proportion of total variance associated with the activity type
IV. Further, we can calculate the effect size index \(\xi^2\), which is based
on Pillai's test statistic, and represents the mean squared canonical correlation
\citep{huberty_applied_2006}:
\[\xi^2 = \frac{U}{r},\] where \(r\) is the number of variates (2, in our case).
Therefore, \(\xi^2 = `r summary(manovaModel, test = 'Pillai') %>% pluck('stats') %>%
.[1,2] %>% {. / 2} -> xsi; xsi`\). Finally, we will calculate Tatsuoka's
\citep[1970; according to][]{huberty_applied_2006} extension of the \(\omega^2\)
to the multivariate case. In this case, \(\omega^2_{mult} = `r 1 - (122 * .875) /
(119 + .875)`\). The adjusted value of the \(\xi^2\) statistic is
\(\xi^2_{adj} = `r 1 - 121/119 * (1 - xsi)`\)

Now, let's take a closer look at the nature of our effect, using linear discriminant
analysis.

### Linear discriminant analysis

```{r ldaCandisc}
. <- candisc(manovaModel, type = '2')
summary(.)
cat('\n======== print ========\n')
.
```

From the above output we can see that the first variate explains most of the variance.
Furthermore, Wilks' lambda values inform us that the groups are separated only
on the first variate, so that's the only one we'll interpret. Also, we can see that
the variation in the grouping variable is almost exclusively explained by the first
variate.

```{r ldaStdCoefs}
.$coeffs.raw %>% as.data.frame(.)
```

```{r ldaStructure}
.$structure %>% as.data.frame(.)
```

Looking at the structure scores, we can see that both the total number of
correct answers and the total number of intrusive distractors chosen share a lot
of variance with the first variate. The first variate is almost completely
defined by the total number of correct answers, but the contribution of the number
of chosen intrusors in also considerable. This could be due to the relatively high
correlation between those two variables.

```{r ldaPlot}
.$scores %>% cbind(datHardNofeed, .) %>%
  ggplot(., aes(x = Can1, y = Can2, color = activityFactor)) +
  geom_point() + geom_vline(aes(xintercept = 0)) +
  geom_point(data = . %>% group_by(activityFactor) %>%
               summarise(ld1 = mean(Can1), ld2 = mean(Can2)),
             mapping = aes(x = ld1, y = ld2),
             color = 'black', size = 3, shape = 3) +
  theme(panel.grid = element_blank()) +
  geom_text_repel(data = . %>% group_by(activityFactor) %>%
               summarise(ld1 = mean(Can1), ld2 = mean(Can2)),
            mapping = aes(x = ld1, y = ld2, label = activityFactor),
            fontface = 'bold',
            inherit.aes = F) +
  scale_color_viridis_d()
```

To assess the ability of the LDA model to discriminate group membership based on
the number of correct answers to the questions and the number of chosen instrusive
distractors, we'll re-train the model and evaluate it's error rate using the
leave-one-out cross-validation technique.

```{r ldaCVTotal}
ldaCVTotal <- lda(activityFactor ~ totalCorrect + totalIntrusors,
                  data = datHardNofeed, CV = T)
confTotal <- caret::confusionMatrix(ldaCVTotal$class,
                                   datHardNofeed$activityFactor %>% as.factor)
confTotal
```

As can be seen from the table, the total LOOCV accuracy is
`r confTotal$overall['Accuracy']`, which is significantly above the
no information rate (which is taken to be the largest class percentage
in the data). According to the Landis & Koch
\citep[1977; as reported in][]{salkind_encyclopedia_2007} guidlines, this
represents only a slight agreement between the predicted and actual classes.
Next, we'll drill into the individual predictors to see which are useful for 
discriminating between different groups.

### Evaluating individual predictors
\citet{tabachnick_using_2012} describe the process of sequential discriminant
analysis, where predictors are entered one-by-one, and the improvement in
classification accuracy is monitored. Therefore, we'll fit an LDA model
containing only the number of correct answers as a predictor. Then, we will
compare this model's LOOCV accuracy to that of the full model (reported at the
end of the previous section).

```{r ldaCVcorrect}
ldaCVcorrect <- lda(activityFactor ~ totalCorrect,
                    data = datHardNofeed, CV = T)
confCorrect <- caret::confusionMatrix(ldaCVcorrect$class,
                                      as.factor(datHardNofeed$activityFactor),
                                      mode = 'everything')
confCorrect
```

As can be seen from the second confusion matrix, the accuracy of this model
is actually somewhat higher than in the full model, as is Cohen's \textkappa.
Importantly, we notice that adding the total number of intrusors to the model
doesn't significantly increase the accuracy of the model (the 95% confidence
intervals for the accuracies of the two models completely overlap).

### Multivariate contrasts

{
    \setstretch{1}
    \bibliographystyle{apacite}
    \bibliography{refs}
}