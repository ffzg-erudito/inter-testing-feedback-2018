\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1.3cm]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={H1-2},
            pdfauthor={DV},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{H1-2}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{DV}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
    \date{}
    \predate{}\postdate{}
  
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\usepackage[natbibapa, sectionbib, tocbib]{apacite}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{bookman}
\usepackage{multirow}
\usepackage{array}
\usepackage[htt]{hyphenat}
\usepackage{booktabs}
\usepackage[euler]{textgreek}
\usepackage{float}
\usepackage[onehalfspacing]{setspace}
\captionsetup[table]{width=\textwidth}
\hypersetup{colorlinks = true, linkcolor = blue, urlcolor = red}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{4}
\tableofcontents
}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(kableExtra)}
\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{dpi =} \DecValTok{600}\NormalTok{, }\DataTypeTok{dev =} \StringTok{'pdf'}\NormalTok{, }\DataTypeTok{echo =}\NormalTok{ F)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## here() starts at /home/denis/Documents/Poso/faks/istra≈æivanja/inter-testing-feedback-2018/analyses
\end{verbatim}

\begin{verbatim}
## -- Attaching packages -------------------------------------------------------- tidyverse 1.2.1 --
\end{verbatim}

\begin{verbatim}
## v ggplot2 3.1.0       v purrr   0.3.1  
## v tibble  2.0.1       v dplyr   0.8.0.1
## v tidyr   0.8.3       v stringr 1.4.0  
## v readr   1.3.1       v forcats 0.4.0
\end{verbatim}

\begin{verbatim}
## -- Conflicts ----------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter()     masks stats::filter()
## x dplyr::group_rows() masks kableExtra::group_rows()
## x dplyr::lag()        masks stats::lag()
\end{verbatim}

\begin{verbatim}
## [conflicted] Will prefer dplyr::filter over any other package
\end{verbatim}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   .default = col_double(),
##   when = col_datetime(format = ""),
##   giveFeedback = col_logical(),
##   condition = col_character(),
##   kolikoProcitaoText1 = col_character(),
##   kolikoProcitaoText2 = col_character(),
##   kolikoProcitaoText3 = col_character(),
##   readingDeficits = col_character(),
##   which = col_character(),
##   readingDifficultiesThisExp = col_character(),
##   activityFactor = col_character()
## )
\end{verbatim}

\begin{verbatim}
## See spec(...) for full column specifications.
\end{verbatim}

\begin{verbatim}
## sROC 0.1-2 loaded
\end{verbatim}

\begin{verbatim}
## Loading required package: car
\end{verbatim}

\begin{verbatim}
## Loading required package: carData
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'car'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     recode
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:purrr':
## 
##     some
\end{verbatim}

\begin{verbatim}
## Loading required package: viridisLite
\end{verbatim}

\begin{verbatim}
## Loading required package: sgeostat
\end{verbatim}

\begin{verbatim}
## [conflicted] Will prefer dplyr::select over any other package
\end{verbatim}

\hypertarget{hard-exclusion-criteria}{%
\section{Hard exclusion criteria}\label{hard-exclusion-criteria}}

The following analyses are going to be conducted on a subset of the
collected data which contains 203 cases. First, we will take a look at
the data going into this analysis. Then, we will check whether the
assumptions for conducting a MANOVA are satisfied. Finally, we will
conduct the analyses specified in the \texttt{analysis-plan.md} file.

\hypertarget{descriptive-statistics}{%
\subsection{Descriptive statistics}\label{descriptive-statistics}}

This analysis is going to be run on a subset participants who were in
no-feedback conditions. This includes the rereading group, and the two
no-feedback test groups.

This leaves us with 122 cases.

\begin{verbatim}
## Observations: 122
## Variables: 3
## $ activityFactor <chr> "content", "general", "rereading", "content", "...
## $ totalCorrect   <dbl> 15, 12, 6, 12, 14, 14, 9, 5, 14, 13, 14, 8, 12,...
## $ totalIntrusors <dbl> 3, 3, 5, 3, 3, 4, 4, 4, 4, 3, 4, 6, 3, 3, 6, 8,...
\end{verbatim}

First of all, it's important to mention that the group sizes are
imbalanced, but the difference is really small:

\begin{verbatim}
## # A tibble: 3 x 2
##   activityFactor     n
##   <chr>          <int>
## 1 content           42
## 2 general           40
## 3 rereading         40
\end{verbatim}

Here are the descriptives for the whole subset.

\begin{verbatim}
##                vars   n  mean   sd median trimmed  mad min max range skew
## totalCorrect      1 122 11.40 3.04     11   11.39 2.97   4  19    15 0.04
## totalIntrusors    2 122  4.18 1.98      4    4.10 1.48   0  10    10 0.38
##                kurtosis   se
## totalCorrect      -0.41 0.28
## totalIntrusors    -0.17 0.18
\end{verbatim}

And the descriptives by group:

\begin{verbatim}
##   condition  n   mean     sd median trimmed    mad min max range      skew
## 1   content 42 12.786 3.0165     12  12.765 2.9652   7  19    12  0.039247
## 2   general 40 10.475 2.8374     10  10.531 2.9652   5  16    11 -0.053301
## 3 rereading 40 10.875 2.8028     11  10.938 2.9652   4  17    13 -0.140828
##   kurtosis      se
## 1 -0.77457 0.46546
## 2 -0.98633 0.44863
## 3 -0.25332 0.44316
\end{verbatim}

\begin{verbatim}
##   condition  n  mean     sd median trimmed    mad min max range    skew
## 1   content 42 3.381 1.6668      3  3.3529 1.4826   0   7     7 0.20311
## 2   general 40 4.575 1.8242      4  4.5312 1.4826   1   9     8 0.32757
## 3 rereading 40 4.625 2.2152      5  4.5312 2.2239   1  10     9 0.27232
##   kurtosis      se
## 1 -0.38484 0.25719
## 2 -0.48422 0.28843
## 3 -0.53693 0.35025
\end{verbatim}

Now, we'll plot the DV distributions, both on the whole subset, and per
group.

\includegraphics{h1-2_files/figure-latex/densityTotal-1.pdf}

The graph shows that there could be significant deviations from
normality. Let's look at the distributions in each group.

\includegraphics{h1-2_files/figure-latex/densityGroup-1.pdf}

The distributions seem to be fairly similar on both dependent variables,
and look a bit more normal than on the whole sample.

Next, here are the boxplots for the three groups, and for both DVs.

\begin{verbatim}
## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{h1-2_files/figure-latex/boxplots-1.pdf}

Looks like there could be an outlier on the number of total intrusions
in the general knowledge test condition.

\hypertarget{assumption-checks}{%
\subsection{Assumption checks}\label{assumption-checks}}

Next, let's look at the correlation between the DVs. First, let's look
at the correlation in the whole sub-sample.

\begin{verbatim}
## Call:psych::corr.test(x = .)
## Correlation matrix 
##                totalCorrect totalIntrusors
## totalCorrect           1.00          -0.67
## totalIntrusors        -0.67           1.00
## Sample Size 
## [1] 122
## Probability values (Entries above the diagonal are adjusted for multiple tests.) 
##                totalCorrect totalIntrusors
## totalCorrect              0              0
## totalIntrusors            0              0
## 
##  To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

We found a statistically significant correlation of -0.66831. Next,
let's look at the correlation in each group.

\begin{verbatim}
## datHardNofeed$activityFactor: content
## Call:FUN(x = data[x, , drop = FALSE])
## Correlation matrix 
##                totalCorrect totalIntrusors
## totalCorrect           1.00          -0.74
## totalIntrusors        -0.74           1.00
## Sample Size 
## [1] 42
## Probability values (Entries above the diagonal are adjusted for multiple tests.) 
##                totalCorrect totalIntrusors
## totalCorrect              0              0
## totalIntrusors            0              0
## 
##  To see confidence intervals of the correlations, print with the short=FALSE option
## -------------------------------------------------------- 
## datHardNofeed$activityFactor: general
## Call:FUN(x = data[x, , drop = FALSE])
## Correlation matrix 
##                totalCorrect totalIntrusors
## totalCorrect           1.00          -0.53
## totalIntrusors        -0.53           1.00
## Sample Size 
## [1] 40
## Probability values (Entries above the diagonal are adjusted for multiple tests.) 
##                totalCorrect totalIntrusors
## totalCorrect              0              0
## totalIntrusors            0              0
## 
##  To see confidence intervals of the correlations, print with the short=FALSE option
## -------------------------------------------------------- 
## datHardNofeed$activityFactor: rereading
## Call:FUN(x = data[x, , drop = FALSE])
## Correlation matrix 
##                totalCorrect totalIntrusors
## totalCorrect           1.00          -0.65
## totalIntrusors        -0.65           1.00
## Sample Size 
## [1] 40
## Probability values (Entries above the diagonal are adjusted for multiple tests.) 
##                totalCorrect totalIntrusors
## totalCorrect              0              0
## totalIntrusors            0              0
## 
##  To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

The correlations in all three groups are fairly similar and
statistically significant. To check whether the relationship between the
DVs could be described as linear, we'll plot the scatterplots for the
whole sample and for each group.

\includegraphics{h1-2_files/figure-latex/scatterToral-1.pdf}

As can be seen from the plot, the relationship seems to be pretty
linear, and the 99\% confidence interval of the regression slopes is
pretty small. Now, let's take a look at the scatterplots for each group
separately.

\includegraphics{h1-2_files/figure-latex/scatterGroup-1.pdf}

Again, we can see that the relationships are linear, as well as similar.
Next, we will check the multivariate normality assumption using the
Henze-Zirkler test.

\begin{verbatim}
## datHardNofeed$activityFactor: content
## $multivariateNormality
##            Test      HZ p value MVN
## 1 Henze-Zirkler 0.23726 0.95885 YES
## 
## $univariateNormality
##           Test       Variable Statistic   p value Normality
## 1 Shapiro-Wilk  totalCorrect     0.9751    0.4830    YES   
## 2 Shapiro-Wilk totalIntrusors    0.9465    0.0483    NO    
## 
## $Descriptives
##                 n   Mean Std.Dev Median Min Max  25th 75th     Skew
## totalCorrect   42 12.786  3.0165     12   7  19 10.25   15 0.039247
## totalIntrusors 42  3.381  1.6668      3   0   7  2.00    5 0.203106
##                Kurtosis
## totalCorrect   -0.77457
## totalIntrusors -0.38484
## 
## -------------------------------------------------------- 
## datHardNofeed$activityFactor: general
## $multivariateNormality
##            Test      HZ p value MVN
## 1 Henze-Zirkler 0.59687 0.23328 YES
## 
## $univariateNormality
##           Test       Variable Statistic   p value Normality
## 1 Shapiro-Wilk  totalCorrect     0.9646    0.2398    YES   
## 2 Shapiro-Wilk totalIntrusors    0.9559    0.1211    YES   
## 
## $Descriptives
##                 n   Mean Std.Dev Median Min Max 25th  75th      Skew
## totalCorrect   40 10.475  2.8374     10   5  16    8 13.00 -0.053301
## totalIntrusors 40  4.575  1.8242      4   1   9    3  5.25  0.327568
##                Kurtosis
## totalCorrect   -0.98633
## totalIntrusors -0.48422
## 
## -------------------------------------------------------- 
## datHardNofeed$activityFactor: rereading
## $multivariateNormality
##            Test     HZ p value MVN
## 1 Henze-Zirkler 0.2667 0.91818 YES
## 
## $univariateNormality
##           Test       Variable Statistic   p value Normality
## 1 Shapiro-Wilk  totalCorrect     0.9858    0.8872    YES   
## 2 Shapiro-Wilk totalIntrusors    0.9665    0.2770    YES   
## 
## $Descriptives
##                 n   Mean Std.Dev Median Min Max 25th 75th     Skew
## totalCorrect   40 10.875  2.8028     11   4  17    9   13 -0.14083
## totalIntrusors 40  4.625  2.2152      5   1  10    3    6  0.27232
##                Kurtosis
## totalCorrect   -0.25332
## totalIntrusors -0.53693
\end{verbatim}

The result of Henze-Zirkler's multivariate normality test shows that a
statistically significant departure from multivariate normality was not
detected. Hence, we will assume that there really is no departure. The
points on the Chi-Square Q-Q plot follow the straight line fairly well,
which is also indicative of a normal distribution.

However, the Shapiro-Wilk test for univariate normality indicates a
departure in the distribution of the total number of intrusive
distractors chosen in one group. It is interesting to notice that the
value of the \emph{W} statistic is close to the maximum of 1, which
indicates a close fit to the normal distribution
\citep{salkind_encyclopedia_2007}. Also, the test is only marginally
significant at the conventional .05 level. Therefore, we will assume
that the data in each group is normally distributed.

Now, we'll take a look at the homogeneity of covariance matrices
assumption. First, let's take a look at the matrices themselves.

\begin{verbatim}
## datHardNofeed$activityFactor: content
##                totalCorrect totalIntrusors
## totalCorrect         9.0993        -3.7456
## totalIntrusors      -3.7456         2.7782
## -------------------------------------------------------- 
## datHardNofeed$activityFactor: general
##                totalCorrect totalIntrusors
## totalCorrect         8.0506        -2.7417
## totalIntrusors      -2.7417         3.3276
## -------------------------------------------------------- 
## datHardNofeed$activityFactor: rereading
##                totalCorrect totalIntrusors
## totalCorrect         7.8558        -4.0224
## totalIntrusors      -4.0224         4.9071
\end{verbatim}

The covariance matrices look quite similar. The ratio of largest to
smallest variance of the number of correct answers is 1.1583. The same
ratio for the number of chosen intrusors is 1.76629. Finally, the ratio
of the largest to the smallest covariance between the two DVs is
0.68159. As we can see, the variance ratios are pretty close to one.
Still, let's test them with Box's M test.

\begin{verbatim}
## 
##  Box's M-test for Homogeneity of Covariance Matrices
## 
## data:  datHardNofeed %>% select(., -activityFactor)
## Chi-Sq (approx.) = 9.34, df = 6, p-value = 0.16
\end{verbatim}

Box's M returns a non-significant p-value, indicating that we cannot
reject the null hypothesis. \citet{field_discovering_2012} and
\citet{raykov_introduction_2008} warn that Box's M is extremely
sensitive, so we'd expect to find a difference if there really was one.
On the other hand, \citet{field_discovering_2012} warn that the test can
return a non-significant p-value when the assumption of multivariate
normality is not tenable. However, given the results of the
Henze-Zirkler multivariate normality test, we suspect that this is not
the case. Since all assumptions seem to hold, we can proceed with the
planned MANOVA.

\hypertarget{note}{%
\subsubsection{Note}\label{note}}

A decision was made not to check the univariate and multivariate
outliers at this point. Regarding the univariate outliers - the boxplots
point to only one case which could be an outlier. The scatterplots show
no point that's obviously different from the rest. As for the
multivariate outliers, \citet{tabachnick_using_2012} warn that the
Mahalanobis distance can produce false negatives or false positives.
Furthermore, deleting a set of outliers and rerunning the analysis can
reveal yet another set of outliers --- without a clear-cut and absolute
criterion, exclusions are somewhat arbitrary. Finally, cases were
excluded based on criteria that are more or less substantivelly
meaningful in the context of the conducted study. Given the above, no
statistical criteria is used for exclusion at this point.

\hypertarget{manova}{%
\subsection{MANOVA}\label{manova}}

Now that it seems that all the assumptions of a MANOVA are satisfied,
let's run the analysis.

\begin{verbatim}
##                            Df Pillai approx F num Df den Df Pr(>F)   
## as.factor(activityFactor)   2  0.126     3.99      4    238 0.0038 **
## Residuals                 119                                        
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{verbatim}
##                            Df Wilks approx F num Df den Df Pr(>F)   
## as.factor(activityFactor)   2 0.875     4.07      4    236 0.0033 **
## Residuals                 119                                       
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{verbatim}
##                            Df Hotelling-Lawley approx F num Df den Df
## as.factor(activityFactor)   2            0.142     4.16      4    234
## Residuals                 119                                        
##                           Pr(>F)   
## as.factor(activityFactor) 0.0028 **
## Residuals                          
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{verbatim}
##                            Df   Roy approx F num Df den Df  Pr(>F)    
## as.factor(activityFactor)   2 0.137     8.13      2    119 0.00049 ***
## Residuals                 119                                         
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

As can be seen from the resulting output, Pillai's V indicates that the
three groups differ significantly along the linear combination of the
two DVs. The other three reported statistics point to the same
conclusion. Therefore, we'll proceed with conducting a linear
discriminant analyis.

We can look at \(1 - \Lambda\) as an extension to the univariate
\(\eta^2\) \citep{huberty_applied_2006}. In our case, the multivariate
\(\eta^2\) is 0.125, which represents the proportion of total variance
associated with the activity type IV. Further, we can calculate the
effect size index \(\xi^2\), which is based on Pillai's test statistic,
and represents the mean squared canonical correlation
\citep{huberty_applied_2006}: \[\xi^2 = \frac{U}{r},\] where \(r\) is
the number of variates (2, in our case). Therefore, \(\xi^2 = 0.06283\).
Finally, we will calculate Tatsuoka's
\citep[1970; according to][]{huberty_applied_2006} extension of the
\(\omega^2\) to the multivariate case. In this case,
\(\omega^2_{mult} = 0.10949\). The adjusted value of the \(\xi^2\)
statistic is \(\xi^2_{adj} = 0.04708\)

Now, let's take a closer look at the nature of our effect, using linear
discriminant analysis.

\hypertarget{linear-discriminant-analysis}{%
\subsubsection{Linear discriminant
analysis}\label{linear-discriminant-analysis}}

\begin{verbatim}
## 
## Canonical Discriminant Analysis for as.factor(activityFactor):
## 
##     CanRsq Eigenvalue Difference Percent Cumulative
## 1 0.120185   0.136602     0.1311   96.13      96.13
## 2 0.005469   0.005499     0.1311    3.87     100.00
## 
## Class means:
## 
##              Can1     Can2
## content    0.5011 -0.01039
## general   -0.3094 -0.08451
## rereading -0.2168  0.09541
## 
##  std coefficients:
##                   Can1  Can2
## totalCorrect    0.7362 1.065
## totalIntrusors -0.3551 1.245
\end{verbatim}

\begin{verbatim}
## 
## ======== print ========
\end{verbatim}

\begin{verbatim}
## 
## Canonical Discriminant Analysis for as.factor(activityFactor):
## 
##    CanRsq Eigenvalue Difference Percent Cumulative
## 1 0.12018     0.1366      0.131   96.13       96.1
## 2 0.00547     0.0055      0.131    3.87      100.0
## 
## Test of H0: The canonical correlations in the 
## current row and all that follow are zero
## 
##   LR test stat approx F numDF denDF Pr(> F)   
## 1        0.875     4.07     4   236  0.0033 **
## 2        0.995     0.65     1   119  0.4202   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

From the above output we can see that the first variate explains most of
the variance. Furthermore, Wilks' lambda values inform us that the
groups are separated only on the first variate, so that's the only one
we'll interpret. Also, we can see that the variation in the grouping
variable is almost exclusively explained by the first variate.

\begin{verbatim}
##                    Can1    Can2
## totalCorrect    0.25479 0.36845
## totalIntrusors -0.18574 0.65094
\end{verbatim}

\begin{verbatim}
##                    Can1    Can2
## totalCorrect    0.96582 0.25920
## totalIntrusors -0.83829 0.54523
\end{verbatim}

Looking at the structure scores, we can see that both the total number
of correct answers and the total number of intrusive distractors chosen
share a lot of variance with the first variate. The first variate is
almost completely defined by the total number of correct answers, but
the contribution of the number of chosen intrusors in also considerable.
This could be due to the relatively high correlation between those two
variables.

\includegraphics{h1-2_files/figure-latex/ldaPlot-1.pdf}

To assess the ability of the LDA model to discriminate group membership
based on the number of correct answers to the questions and the number
of chosen instrusive distractors, we'll re-train the model and evaluate
it's error rate using the leave-one-out cross-validation technique.

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  content general rereading
##   content        27      16        17
##   general        11      19        13
##   rereading       4       5        10
## 
## Overall Statistics
##                                         
##                Accuracy : 0.459         
##                  95% CI : (0.368, 0.552)
##     No Information Rate : 0.344         
##     P-Value [Acc > NIR] : 0.00572       
##                                         
##                   Kappa : 0.185         
##  Mcnemar's Test P-Value : 0.00577       
## 
## Statistics by Class:
## 
##                      Class: content Class: general Class: rereading
## Sensitivity                   0.643          0.475            0.250
## Specificity                   0.588          0.707            0.890
## Pos Pred Value                0.450          0.442            0.526
## Neg Pred Value                0.758          0.734            0.709
## Prevalence                    0.344          0.328            0.328
## Detection Rate                0.221          0.156            0.082
## Detection Prevalence          0.492          0.352            0.156
## Balanced Accuracy             0.615          0.591            0.570
\end{verbatim}

As can be seen from the table, the total LOOCV accuracy is 0.45902,
which is significantly above the no information rate (which is taken to
be the largest class percentage in the data). According to the Landis \&
Koch \citep[1977; as reported in][]{salkind_encyclopedia_2007}
guidlines, this represents only a slight agreement between the predicted
and actual classes. Next, we'll drill into the individual predictors to
see which are useful for discriminating between different groups.

\hypertarget{evaluating-individual-predictors}{%
\subsubsection{Evaluating individual
predictors}\label{evaluating-individual-predictors}}

\citet{tabachnick_using_2012} describe the process of sequential
discriminant analysis, where predictors are entered one-by-one, and the
improvement in classification accuracy is monitored. Therefore, we'll
fit an LDA model containing only the number of correct answers as a
predictor. Then, we will compare this model's LOOCV accuracy to that of
the full model (reported at the end of the previous section).

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  content general rereading
##   content        29      15        16
##   general        11      22        18
##   rereading       2       3         6
## 
## Overall Statistics
##                                        
##                Accuracy : 0.467        
##                  95% CI : (0.376, 0.56)
##     No Information Rate : 0.344        
##     P-Value [Acc > NIR] : 0.00334      
##                                        
##                   Kappa : 0.198        
##  Mcnemar's Test P-Value : 5.87e-05     
## 
## Statistics by Class:
## 
##                      Class: content Class: general Class: rereading
## Sensitivity                   0.690          0.550           0.1500
## Specificity                   0.613          0.646           0.9390
## Pos Pred Value                0.483          0.431           0.5455
## Neg Pred Value                0.790          0.746           0.6937
## Prevalence                    0.344          0.328           0.3279
## Detection Rate                0.238          0.180           0.0492
## Detection Prevalence          0.492          0.418           0.0902
## Balanced Accuracy             0.651          0.598           0.5445
\end{verbatim}

As can be seen from the second confusion matrix, the accuracy of this
model is actually somewhat higher than in the full model, as is Cohen's
\textkappa.

\{ \setstretch{1}

\bibliographystyle{apacite}
    \bibliography{refs}

\}


\end{document}
